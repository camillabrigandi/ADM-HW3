{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM - HW 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    pip install bs4\n",
    "    pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we get a list of all the master's degree courses we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_masters(this_url):\n",
    "  result_url = requests.get(this_url)\n",
    "  result_soup = BeautifulSoup(result_url.text)\n",
    "  result_links = result_soup.find_all('a', {'class':'courseLink'})\n",
    "  result_list=[]\n",
    "  for item in result_links:\n",
    "    result_list.append((item['href'], item.text))\n",
    "\n",
    "  return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the url of th various pages is *https://www.findamasters.com/masters-degrees/msc-degrees/?PG=* followed by the number of the page in the MSc courses catalogue we're in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for page in range(1, 401):\n",
    "    url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='+str(page)\n",
    "    links += extract_masters(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to verify we retrieved all of the needed urls, so we check for the length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the expected length, so we can create and write the required *.txt* file.   \n",
    "Before doing this, I created a new empty file called \"msc_links\" and saved it in the folder where I'm currently working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('msc_links.txt','w')\n",
    "for link in links:\n",
    "\tfile.write(link[0]+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the file it's written in the correct way (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Crawl master's degree pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html(url, dest_folder):\n",
    "    #path is the local path we're working in \n",
    "    if not os.path.exists(dest_folder):  # create folder if it does not exist\n",
    "        os.makedirs(dest_folder) \n",
    "    \n",
    "    url = 'https://www.findamasters.com/'+url[:-2]\n",
    "    filename = url.replace('https://www.findamasters.com//masters-degrees/course/', '').replace('/', '_')\n",
    "    file_path = os.path.join(dest_folder, filename)\n",
    "    url_req = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(url_req.text, 'html.parser')\n",
    "    with open(file_path, 'w') as file: \n",
    "        file.write(soup.prettify())\n",
    "    \n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVING HTML & ORGANIZING THEM IN FOLDERS ###\n",
    "import itertools\n",
    "\n",
    "prova = int(input())\n",
    "with open('msc_links.txt', 'r') as file:\n",
    "    for i in range(1, prova):\n",
    "        folder_i = 'page_' + str(i)\n",
    "        \n",
    "        #selecting the lines corresponding to the i-th page\n",
    "        needed_lines = itertools.islice(file, 0, 15)\n",
    "\n",
    "        #saving the corresponding html in \"page_i\" folder\n",
    "        for url in needed_lines:\n",
    "            download_html(str(url), folder_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
